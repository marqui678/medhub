{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import *\n",
    "import sys, os, numpy, re, string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.stop_words import *\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gettext(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Parse text and remove JavaScript and references\n",
    "    \"\"\"\n",
    "    return text.replace(\"To use the sharing features on this page, please enable JavaScript.\", '').split(\"References\")[0]\n",
    "\n",
    "def tokenize_custom(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    subs = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n' + ']')\n",
    "    processed_text = subs.sub(' ', text)\n",
    "    words = nltk.word_tokenize(processed_text)\n",
    "    words = [word for word in words if len(word) >=3 and word not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "def stemwords(words):\n",
    "    \"\"\"\n",
    "    Given a list of tokens/words, return a new list with each word\n",
    "    stemmed using a PorterStemmer.\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    stemmed_list = [stemmer.stem_word(word) for word in words]\n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer_custom(text):\n",
    "    return stemwords(tokenize_custom(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filelist(root, skip_files ='resources'):\n",
    "    \"\"\"\n",
    "    Get the list of filenames from the medline directory. Ignore filenames that contain 'Resources'\n",
    "    \"\"\"\n",
    "    result =[]\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            if not name.startswith('.') and skip_files not in name:\n",
    "                result.append(os.path.join(path, name))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are functions for loading and saving scipy sparse matrices\n",
    "\n",
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a function to dump the vectorizer object to disk. We need it later to get disease matches.\n",
    "\n",
    "def save_vectorizer(vectorizer, filename = \"diseases_data/vectorizer.pk\"):\n",
    "    with open(filename, 'wb') as fin:\n",
    "        cPickle.dump(vectorizer, fin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_disease_names(filename, filenames_list):\n",
    "    diseases = filenames_list\n",
    "    with open(filename, 'w') as f:\n",
    "        for disease in  diseases:\n",
    "            f.write(\"%s\\n\" % disease)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filelist(\"diseases_data/curated_diseases/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error = 'ignore',input='filename', # argument to transform() is list of files\n",
    "                        analyzer='word',\n",
    "                        tokenizer=tokenizer_custom,  # tokenize, stem\n",
    "                        preprocessor = gettext,\n",
    "                        stop_words='english') # strip out stop words\n",
    "\n",
    "root_dir = \"diseases_data/curated_diseases/\"\n",
    "filenames_list = filelist(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_matrix = tfidf.fit_transform(filenames_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_vectorizer(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr(\"diseases_data/disease_matrix\", doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the order of filenames\n",
    "save_disease_names(\"diseases_data/diseases.txt\", filenames_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
